{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import os.path\n",
    "import math\n",
    "import sys\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from enum import Enum    \n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read file\n",
    "\n",
    "# read csv\n",
    "df = pd.read_csv(\"file.csv\", header=None, sep=\"\\t\") \n",
    "\n",
    "#  read csv chunk\n",
    "num_of_lines = 100000\n",
    "for chunk in pd.read_csv(csv_url, chunksize = num_of_lines):\n",
    "    print(chunk.shape)\n",
    "\n",
    "# read pickle\n",
    "with open('file.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "    \n",
    "# read parquet\n",
    "pd.read_parquet('file.parquet', engine='fastparquet')\n",
    "\n",
    "#read parquet/csv folder recursively\n",
    "def read_folder_into_pd(path, file_type = \"parquet\"):\n",
    "    def get_list_files_in_folder(path, file_type = \"parquet\"):\n",
    "        file_paths =  glob.glob(os.path.join(path, (\"*.\" + file_type)))\n",
    "        if len(file_paths) > 0: # one level - do nothing\n",
    "            pass\n",
    "        else:\n",
    "            sub_folder_paths =  glob.glob(os.path.join(path, \"*\"))\n",
    "            for p in sub_folder_paths:\n",
    "                file_paths += get_list_files_in_folder(p)\n",
    "        return file_paths\n",
    "\n",
    "    file_path_list = get_list_files_in_folder(path, file_type)\n",
    "    if file_type == \"parquet\":\n",
    "        df = pd.concat((pd.read_parquet(f, 'fastparquet') for f in file_path_list))\n",
    "    elif file_type == \"csv\":\n",
    "        df = pd.concat((pd.read_csv(f) for f in file_path_list))\n",
    "        \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### write file\n",
    "\n",
    "# write a list to txt file\n",
    "with open('your_file.txt', 'w') as f:\n",
    "    for item in my_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "# write csv from df\n",
    "df.to_csv(\"filename.csv\", sep=',', index=False)\n",
    "\n",
    "# write pickle\n",
    "with open('filename.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# append to an existing csv file\n",
    "with open(\"accuracy_score.csv\", \"a\") as f:\n",
    "    w = csv.writer(f, delimiter = ' ')\n",
    "    list_to_write = [str(best_score), str(best_estimator), \n",
    "                  str(X.columns), str(reduce_canNotPredict), str(transform_method)]\n",
    "    w.writerow(list_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot\n",
    "\n",
    "# plot numerical variable\n",
    "sns_plot = sns.distplot(age)\n",
    "\n",
    "# plot categorial variable: first, seaborn calculates the num of appearances in each category,then plots, it is equivalent to group_ip.value_counts().plot.bar()\n",
    "sns_plot = sns.countplot(group_ip)\n",
    "\n",
    "# other plots\n",
    "cur_predict_and_label.percent.plot()\n",
    "cur_predict_and_label.percent.plot.bar()\n",
    "\n",
    "\n",
    "## options\n",
    "# rotate 90 degrees\n",
    "sns_plot.set_xticklabels(g.get_xticklabels(), rotation=90) \n",
    "\n",
    "# size\n",
    "plt.figure(figsize = (60,10)) \n",
    "\n",
    "# save to pdf\n",
    "sns_plot.get_figure().savefig(\"output.png\")\n",
    "\n",
    "print(age.describe(percentiles=[i* (1/20) for i in range(20)] ))\n",
    "print(\"skew : \",age_none_zero.skew())\n",
    "print(\"kurt : \", age_none_zero.kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### matrix\n",
    "\n",
    "# confusion matrix\n",
    "cnf_matrix = confusion_matrix(current_prediction, df.label_city_id)\n",
    "pd.crosstab(df.label_city_id, current_prediction, rownames=['Actual Species'], colnames=['Predicted Species'])\n",
    "\n",
    "# heat map\n",
    "def plot_heatmap(col_names, calc_func, df, size = (24,8)):\n",
    "    \n",
    "    plt.figure(figsize = size) \n",
    "    cor_cols = col_names\n",
    "    cor_table = np.zeros((len(cor_cols), len(cor_cols)))\n",
    "\n",
    "    for x, c1 in enumerate(cor_cols):\n",
    "        for y, c2 in enumerate(cor_cols):\n",
    "            cor_table[x][y] = calc_func(df[c1], df[c2])\n",
    "\n",
    "    sns.set(font_scale = 1.25)\n",
    "    hm = sns.heatmap(cor_table, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cor_cols, xticklabels=cor_cols)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix_ver2(df, label_col, pred_col, map_label_to_name, fig_size = (10,7)):\n",
    "    \"\"\"\n",
    "    df: data frame\n",
    "    label_col: label column name\n",
    "    pred_col: predict column name\n",
    "    map_label_to_name: map numerical labels to string names. Let it blank if no need to set.\n",
    "                    example: map_label_to_name = {2: \"ORANGE\", 1:\"APPLE\", 0:\"DURIAN\"}\n",
    "    fig_size: the size of confusion matrix, default: (10,7)\n",
    "    \"\"\"\n",
    "\n",
    "    df_confusion = pd.crosstab(df[label_col], df[pred_col], rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    if map_label_to_name is None:\n",
    "        cols = np.unique(df[label_col])\n",
    "    else:\n",
    "        cols = [map_label_to_name[c] for c in np.unique(df[label_col])]\n",
    "\n",
    "    plt.figure(figsize = fig_size)\n",
    "    sns.set(font_scale=1.0) #for label size\n",
    "    f = sns.heatmap(df_confusion, annot=True,annot_kws={\"size\": 15}, fmt='g', cmap=\"YlGnBu\", xticklabels=cols, yticklabels=cols)# font size\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename\n",
    "\n",
    "path = '../path/'\n",
    "i = 0\n",
    "for filename in os.listdir(path):\n",
    "    os.rename(os.path.join(path,filename), os.path.join(path, filename.replace(\"old_name\", \"new_name\")))\n",
    "    i = i +1\n",
    "for filename in os.listdir(path):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split a col to multi-cols:\n",
    "test_df[hour_by_uid_cols] = pd.DataFrame(test_df._DH_uid.values.tolist()) #fast\n",
    "test_df[hour_by_uid_cols] = test_df._DH_uid.apply(pd.Series) #slow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_3 = df['top_3_city'].map(lambda x: re.sub('\\[|\\]', \"\", str(x)))  #remove [] by \"\" in x\n",
    "df[['city0', 'city1', 'city2']] = top_3.str.split(',', n=2, expand=True)\n",
    "\n",
    "\n",
    "\n",
    "## create new one:\n",
    "# approach 1: (faster approach 2)\n",
    "%%timeit\n",
    "a,b,c = [], [], []\n",
    "for row in df2.itertuples(index=True, name='Pandas'):\n",
    "    t1, t2, t3 = distance(row.long, row.lat)\n",
    "    a.append(t1)\n",
    "    b.append(t2)\n",
    "    c.append(t3)\n",
    "df2['a'] = a\n",
    "df2['b'] = b\n",
    "df2['c'] = c\n",
    "\n",
    "#approach 2:\n",
    "temp = df.apply(convert, axis=1)\n",
    "df[[\"_c\", \"_d\", \"_w\"]] = temp.str.split(\"|\", n=2, expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model light gbm\n",
    "\n",
    "n_jobs = 10\n",
    "data = train_valid_df\n",
    "feature_cols = cols\n",
    "cate_cols =  cate\n",
    "early_stopping = 200\n",
    "num_splits = 4\n",
    "over_sampling = True\n",
    "num_class = data[label].nunique()\n",
    "\n",
    "cv = StratifiedKFold(n_splits = num_splits, random_state = 912, shuffle=True)\n",
    "\n",
    "\n",
    "####################\n",
    "## RUN THE MODEL ##\n",
    "###################\n",
    "\n",
    "## NO need to edit this cell ##\n",
    "\n",
    "start_time = datetime.now()\n",
    "random_seed = 2019\n",
    "\n",
    "\n",
    "print(\"num feature_cols = {} \".format(len(feature_cols)))\n",
    "print(\"num cate cols = {}\".format(len(cate_cols)))\n",
    "## prepare the model\n",
    "lgb_params = {\n",
    "    'boosting_type':'gbdt', 'colsample_bytree':0.75, # 'class_weight': {0 : 1 , 1: weight},\n",
    "    'importance_type':'gain', 'learning_rate':0.005, 'max_depth':5,\n",
    "    \n",
    "    'min_child_samples':20,# 'min_child_weight':0.001, 'min_split_gain':0.0,\n",
    "    'n_estimators':20000, 'n_jobs': n_jobs, 'num_leaves':31, 'subsample_freq':16,\n",
    "    'seed': random_seed, 'reg_alpha':0.0, 'reg_lambda':0.0, 'silent':True,\n",
    "    'subsample':0.5, 'subsample_for_bin':200000, \n",
    "    'metric': ['multi_error', 'multi_logloss'],  #put 'metric' before 'objective', keep multi_error first, multi_logloss second \n",
    "#     \"metric\": [\"ff1\", \"binary_logloss\"] ,\n",
    "    \n",
    "    'objective': 'multi_logloss', 'num_class':num_class\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "oof_list = []\n",
    "for i, (train_index, valid_index) in enumerate(cv.split(data, y=data[label])):\n",
    "    print(\"\\nfold {}/{}\".format(i+1, num_splits))\n",
    "    lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "    X = data[feature_cols]\n",
    "    y = data[label]\n",
    "\n",
    "    # Create data for this fold\n",
    "    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[valid_index,:].copy()\n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[valid_index].copy()\n",
    "    \n",
    "    if over_sampling:\n",
    "        smt = SMOTE(random_state=2019)\n",
    "        X_train, y_train = smt.fit_sample(X_train.replace(np.inf, np.nan).fillna(0), y_train)\n",
    "   \n",
    "\n",
    "\n",
    "    record_store = dict()\n",
    "    lgb_model.fit( X=X_train, y=y_train, feature_name = feature_cols, categorical_feature = cate_cols, \n",
    "                  early_stopping_rounds = early_stopping, eval_set=[(X_train, y_train), (X_valid, y_valid)], \n",
    "                  eval_names=[\"train\", \"valid\"],\n",
    "                  eval_metric= ['multi_error'], verbose = 300) #, callbacks = [lgb.record_evaluation(record_store)])\n",
    "\n",
    "    ## calc loss error for valid set\n",
    "    y_pred = lgb_model.predict(X_valid)\n",
    "    y_pred_prob = lgb_model.predict_proba(X_valid)\n",
    "\n",
    "\n",
    "    oof = pd.concat([X_valid, y_valid], 1)\n",
    "    oof['pred'] = y_pred\n",
    "    oof['prob_0'] = y_pred_prob[:,0]\n",
    "    oof['prob_1'] = y_pred_prob[:,1]\n",
    "\n",
    "    oof_list.append(oof)\n",
    "#     lgb.plot_metric(record_store, figsize=(10,8))\n",
    "#     lgb.plot_metric(record_store, metric=\"multi_logloss\", figsize=(10,8))\n",
    "    \n",
    "#     break\n",
    "    \n",
    "training_time = datetime.now() - start_time\n",
    "\n",
    "print(\"done in {}, using {} cores\".format(training_time, n_jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert categorical variable to numerical one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check size of object\n",
    "import sys\n",
    "for i in dir():\n",
    "    print (i, sys.getsizeof(eval(i)) )\n",
    "\n",
    "df.info(memory_usage=\"deep\")  : more readable\n",
    "\n",
    "sys.getsizeof(df)\n",
    "df.memory_usage(index=True, deep=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import from other folder\n",
    "sys.path.append('/path/to/application/app/importedfolder')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "py3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
